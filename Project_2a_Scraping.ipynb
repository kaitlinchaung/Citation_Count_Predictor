{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping of paper metadata and abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for web scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup_from_url(url):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns beautifulsoup object from given URL\n",
    "    \"\"\"\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    response  = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def scrape_data_from_link_soup(link_soup, link):    \n",
    "        \n",
    "    \"\"\"\n",
    "    Returns all data from paper link \n",
    "    \"\"\"\n",
    " \n",
    "    title = link_soup.find('meta', attrs={'name': 'dc.title'})['content']\n",
    "    date = link_soup.find('time').text\n",
    "    journal = re.sub(r'[0-9:]+', '', link_soup.find('meta', attrs={'name': 'dc.source'})['content']).rstrip()\n",
    "    abstract = link_soup.find('meta', attrs={'name': 'description'})['content']\n",
    "    \n",
    "    authors = []\n",
    "    authors_data = link_soup.findAll('meta', {'name': 'citation_author'})\n",
    "    for author in authors_data:\n",
    "        authors.append(author['content'])\n",
    "    \n",
    "    if len(authors)==0:\n",
    "        first_author = 'NaN'\n",
    "        uni = 'NaN'\n",
    "        PI = 'NaN'\n",
    "    else:   \n",
    "        first_author = authors[0]\n",
    "        PI_data = authors_data[-1]\n",
    "        uni = PI_data.find_next_sibling('meta')['content']\n",
    "        PI = authors[-1]\n",
    "    \n",
    "    metrics_link = link + \"/metrics\"\n",
    "    metrics_soup = get_soup_from_url(metrics_link)\n",
    "    accesses = metrics_soup.find('dl',{\"class\":\"c-article-metrics__access-citation\"}).dt.text.strip()\n",
    "    citations = metrics_soup.find('dl',{\"class\":\"c-article-metrics__access-citation\"}).find_next('dt').find_next('dt').text.strip()\n",
    "    \n",
    "    return([title, accesses, citations, date, journal, first_author, PI, uni, authors, abstract])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_from_page_soup(page_soup): \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns list of links for one page\n",
    "    \"\"\"\n",
    "    \n",
    "    links = []\n",
    "    for line in page_soup.findAll('a',{\"data-track-action\":\"search result\"}):\n",
    "        links.append(\"https://www.nature.com\" + str(line['href']))\n",
    "    return links          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(page_soup):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns all data from all links in one page\n",
    "    \"\"\"\n",
    "    \n",
    "    links = scrape_links_from_page_soup(page_soup)\n",
    "    data_from_one_page = []\n",
    "    for link in links:\n",
    "        link_soup = get_soup_from_url(link)\n",
    "        link_data = scrape_data_from_link_soup(link_soup, link)\n",
    "        data_from_one_page.append(link_data)\n",
    "        print(link)\n",
    "        time.sleep(.5+2*random.random())\n",
    "    return pd.DataFrame(data_from_one_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_pages(start_page, end_page):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape all pages \n",
    "    \"\"\"\n",
    "    \n",
    "    start_page = start_page\n",
    "    all_data = pd.DataFrame()\n",
    "    url = \"https://www.nature.com/search?article_type=protocols%2Cresearch%2Creviews&subject=genetics&page=\" + str(start_page)\n",
    "    while start_page < end_page:\n",
    "        start_page = start_page + 1\n",
    "        page = requests.get(url)\n",
    "        if page.status_code != 200:\n",
    "            break\n",
    "        print(start_page)\n",
    "        url = \"https://www.nature.com/search?article_type=protocols%2Cresearch%2Creviews&subject=genetics&page=\" + str(start_page)\n",
    "        print(url)\n",
    "        page_soup = get_soup_from_url(url)\n",
    "        page_df = scrape_page(page_soup)\n",
    "        all_data = all_data.append(page_df)\n",
    "        time.sleep(.5+2*random.random())\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape data from specified page numbers and save in flat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = scrape_all_pages(250,261)\n",
    "all_data.to_csv(\"paper_data_250_260.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
